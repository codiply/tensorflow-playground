{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import sys\n",
    "import tarfile\n",
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "from scipy import ndimage\n",
    "from six.moves import cPickle as pickle\n",
    "from six.moves.urllib.request import urlretrieve\n",
    "from sklearn.model_selection import StratifiedShuffleSplit"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "top_data_folder = './data/notMNIST/'\n",
    "dataset_small = 'notMNIST_small'\n",
    "dataset_large = 'notMNIST_large'\n",
    "\n",
    "logs_folder = './logs'\n",
    "\n",
    "if not os.path.exists(top_data_folder):\n",
    "    os.makedirs(top_data_folder)\n",
    "    \n",
    "def path_to(f):\n",
    "    return os.path.join(top_data_folder, f)\n",
    "    \n",
    "if not os.path.exists(logs_folder):\n",
    "    os.makedirs(logs_folder)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class DownloadProgress:\n",
    "    def __init__(self):\n",
    "        self.last_percent_reported = None\n",
    "\n",
    "    def __call__(self, count, blockSize, totalSize):\n",
    "        percent = int(count * blockSize * 100 / totalSize)\n",
    "\n",
    "        if self.last_percent_reported != percent:\n",
    "            if percent % 5 == 0:\n",
    "                sys.stdout.write(\"%s%%\" % percent)\n",
    "                sys.stdout.flush()\n",
    "            else:\n",
    "                sys.stdout.write(\".\")\n",
    "                sys.stdout.flush()\n",
    "      \n",
    "            self.last_percent_reported = percent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def download_and_extract(dataset_name):\n",
    "    dataset_archive = dataset_name + '.tar.gz'\n",
    "    filename = path_to(dataset_archive)\n",
    "    if not os.path.exists(filename):\n",
    "        url = 'http://commondatastorage.googleapis.com/books1000/' + dataset_archive\n",
    "        urlretrieve(url, filename, reporthook=DownloadProgress())\n",
    "\n",
    "    letters_folder = path_to(dataset_name)\n",
    "\n",
    "    if not os.path.exists(letters_folder):\n",
    "        with tarfile.open(filename) as tar:\n",
    "            tar.extractall(path=top_data_folder)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "download_and_extract(dataset_small)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "download_and_extract(dataset_large)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "image_size = 28\n",
    "class_count = 10\n",
    "image_channels = 1\n",
    "\n",
    "def load_folder(folder):\n",
    "    image_filenames = [f for f in os.listdir(folder) if not os.path.isdir(os.path.join(folder, f))]\n",
    "    image_count = len(image_filenames)\n",
    "    \n",
    "    dataset = np.ndarray(shape=(image_count, image_size, image_size, image_channels), dtype=np.float32)\n",
    "    \n",
    "    image_counter = 0\n",
    "    \n",
    "    for f in image_filenames:\n",
    "        image_filename = os.path.join(folder, f)\n",
    "        try:\n",
    "            image_data = np.expand_dims(ndimage.imread(image_filename).astype(float), axis=2)\n",
    "            dataset[image_counter, :, :] = image_data\n",
    "            image_counter += 1\n",
    "        except IOError as e:\n",
    "            # There are plenty of images, I skip this one\n",
    "            pass\n",
    "        \n",
    "    dataset = dataset[0:image_counter, :, :]\n",
    "    \n",
    "    print('Full dataset tensor for %s:' % folder, dataset.shape)\n",
    "    \n",
    "    return dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def get_complete_dataset(dataset_name):\n",
    "    image_depth = 256\n",
    "    \n",
    "    pickle_file = path_to(dataset_name + '_conv.pickle')\n",
    "    \n",
    "    if not os.path.exists(pickle_file):\n",
    "        image_pixels = []\n",
    "        image_labels = []\n",
    "        image_labels_one_hot = []\n",
    "        \n",
    "        for i, letter in enumerate(\"ABCDEFGHIJ\"):\n",
    "            letter_dataset = (load_folder(path_to(os.path.join(dataset_name, letter))) * 2 - image_depth) / image_depth\n",
    "            \n",
    "            letter_image_count = len(letter_dataset)\n",
    "            \n",
    "            letter_labels = np.ones(shape=(letter_image_count,), dtype=np.float32) * i\n",
    "            letter_labels_one_hot = np.ndarray(shape=(letter_image_count, class_count), dtype=np.float32)\n",
    "            \n",
    "            label_one_hot = np.zeros(shape = (class_count,), dtype=np.float32)\n",
    "            label_one_hot[i] = 1.0\n",
    "            letter_labels_one_hot[:] = label_one_hot\n",
    "            \n",
    "            image_pixels.append(letter_dataset)\n",
    "            image_labels.append(letter_labels)\n",
    "            image_labels_one_hot.append(letter_labels_one_hot)\n",
    "            \n",
    "        dataset = { 'pixels': np.concatenate(image_pixels), \n",
    "                    'labels': np.concatenate(image_labels),\n",
    "                    'labels_one_hot': np.concatenate(image_labels_one_hot) }\n",
    "            \n",
    "        with open(pickle_file, 'wb') as f:\n",
    "            pickle.dump(dataset, f, pickle.HIGHEST_PROTOCOL)\n",
    "            \n",
    "        return dataset\n",
    "    else:\n",
    "        with open(pickle_file, 'rb') as f:\n",
    "            return pickle.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def split(dataset_name, test_size=0.2, validate_size=0.25, random_state=42):\n",
    "    dataset = get_complete_dataset(dataset_name)\n",
    "    answer = {}\n",
    "    \n",
    "    sss1 = StratifiedShuffleSplit(n_splits=1, test_size=test_size, random_state=random_state)\n",
    "    train_validate_indices, test_indices = list(sss1.split(dataset['pixels'], dataset['labels']))[0]\n",
    "\n",
    "    answer['X_test'] = dataset['pixels'][test_indices]\n",
    "    answer['y_test'] = dataset['labels_one_hot'][test_indices]\n",
    "\n",
    "    train_validate_pixels = dataset['pixels'][train_validate_indices]\n",
    "    train_validate_labels = dataset['labels'][train_validate_indices]\n",
    "    train_validate_labels_one_hot = dataset['labels_one_hot'][train_validate_indices]\n",
    "    \n",
    "    sss2 = StratifiedShuffleSplit(n_splits=1, test_size=validate_size, random_state=random_state * random_state)\n",
    "    train_indices, validate_indices = list(sss2.split(train_validate_pixels, train_validate_labels_one_hot))[0]\n",
    "    \n",
    "    answer['X_train'] = train_validate_pixels[train_indices]\n",
    "    answer['y_train'] = train_validate_labels_one_hot[train_indices]\n",
    "    \n",
    "    answer['X_validate'] = train_validate_pixels[validate_indices]\n",
    "    answer['y_validate'] = train_validate_labels_one_hot[validate_indices]\n",
    "    \n",
    "    return answer    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "split_dataset = split(dataset_small)\n",
    "#split_dataset = split(dataset_large)d"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "('X_test', (3745, 28, 28, 1))\n",
      "('X_train', (11234, 28, 28, 1))\n",
      "('X_validate', (3745, 28, 28, 1))\n",
      "('y_validate', (3745, 10))\n",
      "('y_train', (11234, 10))\n",
      "('y_test', (3745, 10))\n"
     ]
    }
   ],
   "source": [
    "for k in split_dataset.keys():\n",
    "    print(k, split_dataset[k].shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# I try to make it overfit\n",
    "batch_size = split_dataset['X_train'].shape[0] / 2\n",
    "learning_rate = 0.8\n",
    "\n",
    "patch_size_1 = 7\n",
    "depth_1 = 16\n",
    "\n",
    "patch_size_2 = 5\n",
    "depth_2 = 8\n",
    "\n",
    "stride_1 = 2\n",
    "strides_1 = [1, stride_1, stride_1, 1]\n",
    "\n",
    "stride_2 = 2\n",
    "strides_2 = [1, stride_2, stride_2, 1]\n",
    "\n",
    "hidden_node_count= 64"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "graph = tf.Graph()\n",
    "\n",
    "with graph.as_default():\n",
    "    # Placeholders for train datasets/labels\n",
    "    tf_train_dataset = tf.placeholder(tf.float32, shape=(batch_size, image_size, image_size, image_channels),\n",
    "                                     name='train_dataset')\n",
    "    tf_train_labels = tf.placeholder(tf.float32, shape=(batch_size, class_count), name='train_labels')\n",
    "    \n",
    "    # Constants for validate/test data\n",
    "    tf_validate_dataset = tf.constant(split_dataset['X_validate'], name='validate_dataset')\n",
    "    tf_validate_labels = tf.constant(split_dataset['y_validate'], name='validate_labels')\n",
    "    tf_test_dataset = tf.constant(split_dataset['X_test'], name='test_dataset')\n",
    "    tf_test_labels = tf.constant(split_dataset['y_test'], name='test_labels')\n",
    "    \n",
    "    dropout_keep_prob = tf.placeholder(tf.float32) \n",
    "    \n",
    "    # Variables\n",
    "    layer_1_conv_filter = tf.Variable(tf.truncated_normal([patch_size_1, patch_size_1, image_channels, depth_1], stddev=0.1),\n",
    "                                     name='layer_1_conv_filter')\n",
    "    layer_1_biases = tf.Variable(tf.zeros([depth_1]), name='layer_1_biases')\n",
    "    layer_2_conv_filter = tf.Variable(tf.truncated_normal([patch_size_2, patch_size_2, depth_1, depth_2], stddev=0.1), \n",
    "                                      name='layer_2_conv_filter')\n",
    "    layer_2_biases = tf.Variable(tf.ones([depth_2]), name='layer_2_biases')\n",
    "    \n",
    "    layer_1_output_size = image_size // stride_1\n",
    "    layer_2_output_size = layer_1_output_size // stride_2\n",
    "    \n",
    "    layer_3_weights = tf.Variable(tf.truncated_normal([layer_2_output_size * layer_2_output_size * depth_2, hidden_node_count],\n",
    "                                                     stddev=0.1), name='layer_3_weights')\n",
    "    layer_3_biases = tf.Variable(tf.ones([hidden_node_count]), name='layer_3_biases')\n",
    "    \n",
    "    layer_4_weights = tf.Variable(tf.truncated_normal([hidden_node_count, class_count], stddev=0.1), name='layer_4_weights')\n",
    "    layer_4_biases = tf.Variable(tf.ones(class_count), name='layer_4_biases')\n",
    "    \n",
    "    one = tf.constant(1.0)\n",
    "    \n",
    "    # Model\n",
    "    def model(data, keep_prob=one):\n",
    "        conv1 = tf.nn.conv2d(data, layer_1_conv_filter, strides_1, padding='SAME')\n",
    "        hidden1 = tf.nn.relu(conv1 + layer_1_biases)\n",
    "        \n",
    "        hidden1 = tf.cond(tf.less(keep_prob, one), lambda: tf.nn.dropout(hidden1, keep_prob), lambda: hidden1)\n",
    "        \n",
    "        conv2 = tf.nn.conv2d(hidden1, layer_2_conv_filter, strides_2, padding='SAME')\n",
    "        hidden2 = tf.nn.relu(conv2 + layer_2_biases)\n",
    "        \n",
    "        hidden2 = tf.cond(tf.less(keep_prob, one), lambda: tf.nn.dropout(hidden2, keep_prob), lambda: hidden2)\n",
    "        \n",
    "        hidden2_shape = hidden2.get_shape().as_list()\n",
    "        hidden2_reshaped = tf.reshape(hidden2, [hidden2_shape[0], hidden2_shape[1] * hidden2_shape[2] * hidden2_shape[3]])\n",
    "        hidden3 = tf.nn.relu(tf.matmul(hidden2_reshaped, layer_3_weights) + layer_3_biases)\n",
    "        hidden3 = tf.cond(tf.less(keep_prob, one), lambda: tf.nn.dropout(hidden3, keep_prob), lambda: hidden3)\n",
    "            \n",
    "        output = tf.nn.relu(tf.matmul(hidden3, layer_4_weights) + layer_4_biases)\n",
    "        return output\n",
    "       \n",
    "    # Train\n",
    "    logits_train = model(tf_train_dataset, dropout_keep_prob)\n",
    "    loss = tf.reduce_mean(tf.nn.softmax_cross_entropy_with_logits(logits=logits_train, labels=tf_train_labels))\n",
    "\n",
    "    optimizer = tf.train.GradientDescentOptimizer(learning_rate).minimize(loss)\n",
    "  \n",
    "    # Predictions\n",
    "    prediction_train = tf.nn.softmax(logits_train)\n",
    "    prediction_validate = tf.nn.softmax(model(tf_validate_dataset))\n",
    "    prediction_test = tf.nn.softmax(model(tf_test_dataset))\n",
    "    \n",
    "    def accuracy(predictions, labels):\n",
    "        correct_predictions = tf.equal(tf.argmax(predictions, 1), tf.argmax(labels, 1))\n",
    "        return 100.0 * tf.reduce_mean(tf.cast(correct_predictions, tf.float32))\n",
    "    \n",
    "    # Accuracies\n",
    "    accuracy_train = accuracy(prediction_train, tf_train_labels)\n",
    "    accuracy_validate = accuracy(prediction_validate, tf_validate_labels)\n",
    "    accuracy_test = accuracy(prediction_test, tf_test_labels)\n",
    "    \n",
    "    # Summaries\n",
    "    tf.scalar_summary(\"Minibatch Loss\", loss)\n",
    "    tf.scalar_summary(\"Minibatch Accuracy\", accuracy_validate)\n",
    "    tf.scalar_summary(\"Validate Accuracy\", accuracy_validate)\n",
    "    tf.scalar_summary(\"Test Accuracy\", accuracy_test)\n",
    "    \n",
    "    summary_op = tf.merge_all_summaries()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": false,
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "num_steps = 1001\n",
    "\n",
    "def train(keep_prob=1.0):\n",
    "    with tf.Session(graph=graph) as session:\n",
    "        writer = tf.train.SummaryWriter(logs_folder, graph=graph)\n",
    "    \n",
    "        session.run(tf.initialize_all_variables())\n",
    "        print(\"Initialized\")\n",
    "    \n",
    "        for step in range(num_steps):\n",
    "            offset = (step * batch_size) % (split_dataset['y_train'].shape[0] - batch_size)\n",
    "    \n",
    "            batch_data = split_dataset['X_train'][offset:(offset + batch_size), :]\n",
    "            batch_labels = split_dataset['y_train'][offset:(offset + batch_size), :]\n",
    "    \n",
    "            feed_dict = {tf_train_dataset : batch_data, \n",
    "                         tf_train_labels : batch_labels,\n",
    "                         dropout_keep_prob : keep_prob}\n",
    "            _, l, summary, acc_tr, acc_val, acc_te = session.run(\n",
    "                [optimizer, loss, summary_op, \n",
    "                 accuracy_train, accuracy_validate, accuracy_test], \n",
    "                feed_dict=feed_dict)\n",
    "        \n",
    "            writer.add_summary(summary, step)\n",
    "        \n",
    "            if (step % 100 == 0):\n",
    "                print(\"Minibatch loss at step %d: %f\" % (step, l))\n",
    "                print(\"Minibatch accuracy: %.2f%%\" % acc_tr)\n",
    "                print(\"Validation accuracy: %.2f%%\" % acc_val)\n",
    "        print(\"Test accuracy: %.2f%%\" % acc_te)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Initialized\n",
      "Minibatch loss at step 0: 2.861829\n",
      "Minibatch accuracy: 8.88%\n",
      "Validation accuracy: 8.49%\n",
      "Minibatch loss at step 100: 2.302563\n",
      "Minibatch accuracy: 10.33%\n",
      "Validation accuracy: 10.28%\n",
      "Minibatch loss at step 200: 2.302563\n",
      "Minibatch accuracy: 10.33%\n",
      "Validation accuracy: 10.28%\n",
      "Minibatch loss at step 300: 2.302563\n",
      "Minibatch accuracy: 10.33%\n",
      "Validation accuracy: 10.28%\n",
      "Minibatch loss at step 400: 2.302563\n",
      "Minibatch accuracy: 10.33%\n",
      "Validation accuracy: 10.28%\n",
      "Minibatch loss at step 500: 2.302563\n",
      "Minibatch accuracy: 10.33%\n",
      "Validation accuracy: 10.28%\n",
      "Minibatch loss at step 600: 2.302569\n",
      "Minibatch accuracy: 10.33%\n",
      "Validation accuracy: 10.28%\n",
      "Minibatch loss at step 700: 2.302563\n",
      "Minibatch accuracy: 10.33%\n",
      "Validation accuracy: 10.28%\n",
      "Minibatch loss at step 800: 2.302563\n",
      "Minibatch accuracy: 10.33%\n",
      "Validation accuracy: 10.28%\n",
      "Minibatch loss at step 900: 2.302563\n",
      "Minibatch accuracy: 10.33%\n",
      "Validation accuracy: 10.28%\n",
      "Minibatch loss at step 1000: 2.302563\n",
      "Minibatch accuracy: 10.33%\n",
      "Validation accuracy: 10.28%\n",
      "Test accuracy: 9.99%\n"
     ]
    }
   ],
   "source": [
    "train(0.9)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Initialized\n",
      "Minibatch loss at step 0: 4.767598\n",
      "Minibatch accuracy: 9.77%\n",
      "Validation accuracy: 9.16%\n",
      "Minibatch loss at step 100: 2.303085\n",
      "Minibatch accuracy: 10.61%\n",
      "Validation accuracy: 10.28%\n",
      "Minibatch loss at step 200: 2.259469\n",
      "Minibatch accuracy: 16.54%\n",
      "Validation accuracy: 18.10%\n",
      "Minibatch loss at step 300: 1.692468\n",
      "Minibatch accuracy: 40.63%\n",
      "Validation accuracy: 46.09%\n",
      "Minibatch loss at step 400: 1.626614\n",
      "Minibatch accuracy: 42.94%\n",
      "Validation accuracy: 46.92%\n",
      "Minibatch loss at step 500: 1.594313\n",
      "Minibatch accuracy: 43.94%\n",
      "Validation accuracy: 47.32%\n",
      "Minibatch loss at step 600: 1.569720\n",
      "Minibatch accuracy: 44.53%\n",
      "Validation accuracy: 47.61%\n",
      "Minibatch loss at step 700: 1.567476\n",
      "Minibatch accuracy: 44.83%\n",
      "Validation accuracy: 47.93%\n",
      "Minibatch loss at step 800: 1.549823\n",
      "Minibatch accuracy: 45.49%\n",
      "Validation accuracy: 48.09%\n",
      "Minibatch loss at step 900: 1.535314\n",
      "Minibatch accuracy: 45.54%\n",
      "Validation accuracy: 48.44%\n",
      "Minibatch loss at step 1000: 1.529958\n",
      "Minibatch accuracy: 45.84%\n",
      "Validation accuracy: 48.46%\n",
      "Test accuracy: 46.46%\n"
     ]
    }
   ],
   "source": [
    "train(0.5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Initialized\n",
      "Minibatch loss at step 0: 20.313150\n",
      "Minibatch accuracy: 10.29%\n",
      "Validation accuracy: 9.37%\n",
      "Minibatch loss at step 100: 2.302680\n",
      "Minibatch accuracy: 10.20%\n",
      "Validation accuracy: 10.28%\n",
      "Minibatch loss at step 200: 2.302757\n",
      "Minibatch accuracy: 10.47%\n",
      "Validation accuracy: 10.28%\n",
      "Minibatch loss at step 300: 2.302331\n",
      "Minibatch accuracy: 10.40%\n",
      "Validation accuracy: 10.28%\n",
      "Minibatch loss at step 400: 2.302730\n",
      "Minibatch accuracy: 10.29%\n",
      "Validation accuracy: 10.28%\n",
      "Minibatch loss at step 500: 2.302505\n",
      "Minibatch accuracy: 10.49%\n",
      "Validation accuracy: 10.28%\n",
      "Minibatch loss at step 600: 2.302767\n",
      "Minibatch accuracy: 10.24%\n",
      "Validation accuracy: 10.28%\n",
      "Minibatch loss at step 700: 2.302639\n",
      "Minibatch accuracy: 10.27%\n",
      "Validation accuracy: 10.28%\n",
      "Minibatch loss at step 800: 2.302946\n",
      "Minibatch accuracy: 10.20%\n",
      "Validation accuracy: 10.28%\n",
      "Minibatch loss at step 900: 2.302620\n",
      "Minibatch accuracy: 10.34%\n",
      "Validation accuracy: 10.28%\n",
      "Minibatch loss at step 1000: 2.302437\n",
      "Minibatch accuracy: 10.33%\n",
      "Validation accuracy: 10.28%\n",
      "Test accuracy: 9.99%\n"
     ]
    }
   ],
   "source": [
    "train(0.1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
