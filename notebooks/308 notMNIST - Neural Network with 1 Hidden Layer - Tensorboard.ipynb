{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import sys\n",
    "import tarfile\n",
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "from scipy import ndimage\n",
    "from six.moves import cPickle as pickle\n",
    "from six.moves.urllib.request import urlretrieve\n",
    "from sklearn.model_selection import StratifiedShuffleSplit"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "top_data_folder = './data/notMNIST/'\n",
    "dataset_small = 'notMNIST_small'\n",
    "dataset_large = 'notMNIST_large'\n",
    "\n",
    "logs_folder = './logs'\n",
    "\n",
    "if not os.path.exists(top_data_folder):\n",
    "    os.makedirs(top_data_folder)\n",
    "    \n",
    "def path_to(f):\n",
    "    return os.path.join(top_data_folder, f)\n",
    "    \n",
    "if not os.path.exists(logs_folder):\n",
    "    os.makedirs(logs_folder)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class DownloadProgress:\n",
    "    def __init__(self):\n",
    "        self.last_percent_reported = None\n",
    "\n",
    "    def __call__(self, count, blockSize, totalSize):\n",
    "        percent = int(count * blockSize * 100 / totalSize)\n",
    "\n",
    "        if self.last_percent_reported != percent:\n",
    "            if percent % 5 == 0:\n",
    "                sys.stdout.write(\"%s%%\" % percent)\n",
    "                sys.stdout.flush()\n",
    "            else:\n",
    "                sys.stdout.write(\".\")\n",
    "                sys.stdout.flush()\n",
    "      \n",
    "            self.last_percent_reported = percent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def download_and_extract(dataset_name):\n",
    "    dataset_archive = dataset_name + '.tar.gz'\n",
    "    filename = path_to(dataset_archive)\n",
    "    if not os.path.exists(filename):\n",
    "        url = 'http://commondatastorage.googleapis.com/books1000/' + dataset_archive\n",
    "        urlretrieve(url, filename, reporthook=DownloadProgress())\n",
    "\n",
    "    letters_folder = path_to(dataset_name)\n",
    "\n",
    "    if not os.path.exists(letters_folder):\n",
    "        with tarfile.open(filename) as tar:\n",
    "            tar.extractall(path=top_data_folder)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "download_and_extract(dataset_small)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "download_and_extract(dataset_large)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "image_size = 28\n",
    "image_pixels = image_size * image_size\n",
    "class_count = 10\n",
    "\n",
    "def load_folder(folder):\n",
    "    image_filenames = [f for f in os.listdir(folder) if not os.path.isdir(os.path.join(folder, f))]\n",
    "    image_count = len(image_filenames)\n",
    "    \n",
    "    dataset = np.ndarray(shape=(image_count, image_pixels), dtype=np.float32)\n",
    "    \n",
    "    image_counter = 0\n",
    "    \n",
    "    for f in image_filenames:\n",
    "        image_filename = os.path.join(folder, f)\n",
    "        try:\n",
    "            image_data = np.ndarray.flatten(ndimage.imread(image_filename).astype(float))\n",
    "            dataset[image_counter, :] = image_data\n",
    "            image_counter += 1\n",
    "        except IOError as e:\n",
    "            # There are plenty of images, I skip this one\n",
    "            pass\n",
    "        \n",
    "    dataset = dataset[0:image_counter, :]\n",
    "    \n",
    "    print('Full dataset tensor for %s:' % folder, dataset.shape)\n",
    "    \n",
    "    return dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def get_complete_dataset(dataset_name):\n",
    "    image_depth = 256\n",
    "    \n",
    "    pickle_file = path_to(dataset_name + '.pickle')\n",
    "    \n",
    "    if not os.path.exists(pickle_file):\n",
    "        image_pixels = []\n",
    "        image_labels = []\n",
    "        image_labels_one_hot = []\n",
    "        \n",
    "        for i, letter in enumerate(\"ABCDEFGHIJ\"):\n",
    "            letter_dataset = (load_folder(path_to(os.path.join(dataset_name, letter))) * 2 - image_depth) / image_depth\n",
    "            \n",
    "            letter_image_count = len(letter_dataset)\n",
    "            \n",
    "            letter_labels = np.ones(shape=(letter_image_count,), dtype=np.float32) * i\n",
    "            letter_labels_one_hot = np.ndarray(shape=(letter_image_count, class_count), dtype=np.float32)\n",
    "            \n",
    "            label_one_hot = np.zeros(shape = (class_count,), dtype=np.float32)\n",
    "            label_one_hot[i] = 1.0\n",
    "            letter_labels_one_hot[:] = label_one_hot\n",
    "            \n",
    "            image_pixels.append(letter_dataset)\n",
    "            image_labels.append(letter_labels)\n",
    "            image_labels_one_hot.append(letter_labels_one_hot)\n",
    "            \n",
    "        dataset = { 'pixels': np.concatenate(image_pixels), \n",
    "                    'labels': np.concatenate(image_labels),\n",
    "                    'labels_one_hot': np.concatenate(image_labels_one_hot) }\n",
    "            \n",
    "        with open(pickle_file, 'wb') as f:\n",
    "            pickle.dump(dataset, f, pickle.HIGHEST_PROTOCOL)\n",
    "            \n",
    "        return dataset\n",
    "    else:\n",
    "        with open(pickle_file, 'rb') as f:\n",
    "            return pickle.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def split(dataset_name, test_size=0.2, validate_size=0.25, random_state=42):\n",
    "    dataset = get_complete_dataset(dataset_name)\n",
    "    answer = {}\n",
    "    \n",
    "    sss1 = StratifiedShuffleSplit(n_splits=1, test_size=test_size, random_state=random_state)\n",
    "    train_validate_indices, test_indices = list(sss1.split(dataset['pixels'], dataset['labels']))[0]\n",
    "\n",
    "    answer['X_test'] = dataset['pixels'][test_indices]\n",
    "    answer['y_test'] = dataset['labels_one_hot'][test_indices]\n",
    "\n",
    "    train_validate_pixels = dataset['pixels'][train_validate_indices]\n",
    "    train_validate_labels = dataset['labels'][train_validate_indices]\n",
    "    train_validate_labels_one_hot = dataset['labels_one_hot'][train_validate_indices]\n",
    "    \n",
    "    sss2 = StratifiedShuffleSplit(n_splits=1, test_size=validate_size, random_state=random_state * random_state)\n",
    "    train_indices, validate_indices = list(sss2.split(train_validate_pixels, train_validate_labels_one_hot))[0]\n",
    "    \n",
    "    answer['X_train'] = train_validate_pixels[train_indices]\n",
    "    answer['y_train'] = train_validate_labels_one_hot[train_indices]\n",
    "    \n",
    "    answer['X_validate'] = train_validate_pixels[validate_indices]\n",
    "    answer['y_validate'] = train_validate_labels_one_hot[validate_indices]\n",
    "    \n",
    "    return answer    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "split_dataset = split(dataset_small)\n",
    "#split_dataset = split(dataset_large)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "('X_test', (3745, 784))\n",
      "('X_train', (11234, 784))\n",
      "('X_validate', (3745, 784))\n",
      "('y_validate', (3745, 10))\n",
      "('y_train', (11234, 10))\n",
      "('y_test', (3745, 10))\n"
     ]
    }
   ],
   "source": [
    "for k in split_dataset.keys():\n",
    "    print(k, split_dataset[k].shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "batch_size = 128\n",
    "hidden_node_count = 1024\n",
    "learning_rate = 0.01\n",
    "\n",
    "def accuracy(predictions, labels):\n",
    "    correct_predictions = tf.equal(tf.argmax(predictions, 1), tf.argmax(labels, 1))\n",
    "    return 100.0 * tf.reduce_mean(tf.cast(correct_predictions, tf.float32))\n",
    "\n",
    "graph = tf.Graph()\n",
    "\n",
    "with graph.as_default():\n",
    "    tf_train_dataset = tf.placeholder(tf.float32, shape=(batch_size, image_pixels))\n",
    "    tf_train_labels = tf.placeholder(tf.float32, shape=(batch_size, class_count))\n",
    "    tf_validate_dataset = tf.constant(split_dataset['X_validate'])\n",
    "    tf_validate_labels = tf.constant(split_dataset['y_validate'])\n",
    "    tf_test_dataset = tf.constant(split_dataset['X_test'])\n",
    "    tf_test_labels = tf.constant(split_dataset['y_test'])\n",
    "    \n",
    "    weights_0 = tf.Variable(tf.truncated_normal([image_pixels, hidden_node_count]))\n",
    "    biases_0 = tf.Variable(tf.zeros([hidden_node_count]))\n",
    "    weights_1 = tf.Variable(tf.truncated_normal([hidden_node_count, class_count]))\n",
    "    biases_1 =tf.Variable(tf.zeros([class_count]))\n",
    "\n",
    "    hidden_train = tf.nn.relu(tf.matmul(tf_train_dataset, weights_0) + biases_0)\n",
    "    logits_train = tf.matmul(hidden_train, weights_1) + biases_1\n",
    "    loss = tf.reduce_mean(tf.nn.softmax_cross_entropy_with_logits(logits_train, tf_train_labels))\n",
    "\n",
    "    optimizer = tf.train.GradientDescentOptimizer(learning_rate).minimize(loss)\n",
    "  \n",
    "    prediction_train = tf.nn.softmax(logits_train)\n",
    "    \n",
    "    hidden_validate = tf.nn.relu(tf.matmul(tf_validate_dataset, weights_0) + biases_0)\n",
    "    prediction_validate = tf.nn.softmax(tf.matmul(hidden_validate, weights_1) + biases_1)\n",
    "    \n",
    "    hidden_test = tf.nn.relu(tf.matmul(tf_test_dataset, weights_0) + biases_0)\n",
    "    prediction_test = tf.nn.softmax(tf.matmul(hidden_test, weights_1) + biases_1)\n",
    "    \n",
    "    accuracy_train = accuracy(prediction_train, tf_train_labels)\n",
    "    accuracy_validate = accuracy(prediction_validate, tf_validate_labels)\n",
    "    accuracy_test = accuracy(prediction_test, tf_test_labels)\n",
    "    \n",
    "    tf.scalar_summary(\"Minibatch Loss\", loss)\n",
    "    tf.scalar_summary(\"Minibatch Accuracy\", accuracy_validate)\n",
    "    tf.scalar_summary(\"Validate Accuracy\", accuracy_validate)\n",
    "    tf.scalar_summary(\"Test Accuracy\", accuracy_test)\n",
    "    \n",
    "    summary_op = tf.merge_all_summaries()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": false,
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Initialized\n",
      "Minibatch loss at step 0: 640.280334\n",
      "Minibatch accuracy: 12.5%\n",
      "Validation accuracy: 12.7%\n",
      "Minibatch loss at step 1000: 12.037876\n",
      "Minibatch accuracy: 93.8%\n",
      "Validation accuracy: 88.5%\n",
      "Minibatch loss at step 2000: 0.807581\n",
      "Minibatch accuracy: 96.1%\n",
      "Validation accuracy: 91.3%\n",
      "Minibatch loss at step 3000: 0.000766\n",
      "Minibatch accuracy: 100.0%\n",
      "Validation accuracy: 92.5%\n",
      "Minibatch loss at step 4000: 0.000162\n",
      "Minibatch accuracy: 100.0%\n",
      "Validation accuracy: 93.0%\n",
      "Minibatch loss at step 5000: 0.000005\n",
      "Minibatch accuracy: 100.0%\n",
      "Validation accuracy: 93.1%\n",
      "Test accuracy: 84.6%\n"
     ]
    }
   ],
   "source": [
    "num_steps = 5001\n",
    "\n",
    "with tf.Session(graph=graph) as session:\n",
    "    writer = tf.train.SummaryWriter(logs_folder, graph=graph)\n",
    "    \n",
    "    session.run(tf.initialize_all_variables())\n",
    "    print(\"Initialized\")\n",
    "    \n",
    "    for step in range(num_steps):\n",
    "        offset = (step * batch_size) % (split_dataset['y_train'].shape[0] - batch_size)\n",
    "    \n",
    "        batch_data = split_dataset['X_train'][offset:(offset + batch_size), :]\n",
    "        batch_labels = split_dataset['y_train'][offset:(offset + batch_size), :]\n",
    "    \n",
    "        feed_dict = {tf_train_dataset : batch_data, tf_train_labels : batch_labels}\n",
    "        _, l, predictions, summary, acc_tr, acc_val, acc_te = session.run(\n",
    "            [optimizer, loss, prediction_train, summary_op, \n",
    "             accuracy_train, accuracy_validate, accuracy_test], \n",
    "            feed_dict=feed_dict)\n",
    "        \n",
    "        writer.add_summary(summary, step)\n",
    "        \n",
    "        if (step % 1000 == 0):\n",
    "            print(\"Minibatch loss at step %d: %f\" % (step, l))\n",
    "            print(\"Minibatch accuracy: %.1f%%\" % acc_tr)\n",
    "            print(\"Validation accuracy: %.1f%%\" % acc_val)\n",
    "    print(\"Test accuracy: %.1f%%\" % acc_te)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
