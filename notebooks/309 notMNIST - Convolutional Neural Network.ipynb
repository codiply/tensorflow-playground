{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import sys\n",
    "import tarfile\n",
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "from scipy import ndimage\n",
    "from six.moves import cPickle as pickle\n",
    "from six.moves.urllib.request import urlretrieve\n",
    "from sklearn.model_selection import StratifiedShuffleSplit"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "top_data_folder = './data/notMNIST/'\n",
    "dataset_small = 'notMNIST_small'\n",
    "dataset_large = 'notMNIST_large'\n",
    "\n",
    "logs_folder = './logs'\n",
    "\n",
    "if not os.path.exists(top_data_folder):\n",
    "    os.makedirs(top_data_folder)\n",
    "    \n",
    "def path_to(f):\n",
    "    return os.path.join(top_data_folder, f)\n",
    "    \n",
    "if not os.path.exists(logs_folder):\n",
    "    os.makedirs(logs_folder)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class DownloadProgress:\n",
    "    def __init__(self):\n",
    "        self.last_percent_reported = None\n",
    "\n",
    "    def __call__(self, count, blockSize, totalSize):\n",
    "        percent = int(count * blockSize * 100 / totalSize)\n",
    "\n",
    "        if self.last_percent_reported != percent:\n",
    "            if percent % 5 == 0:\n",
    "                sys.stdout.write(\"%s%%\" % percent)\n",
    "                sys.stdout.flush()\n",
    "            else:\n",
    "                sys.stdout.write(\".\")\n",
    "                sys.stdout.flush()\n",
    "      \n",
    "            self.last_percent_reported = percent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def download_and_extract(dataset_name):\n",
    "    dataset_archive = dataset_name + '.tar.gz'\n",
    "    filename = path_to(dataset_archive)\n",
    "    if not os.path.exists(filename):\n",
    "        url = 'http://commondatastorage.googleapis.com/books1000/' + dataset_archive\n",
    "        urlretrieve(url, filename, reporthook=DownloadProgress())\n",
    "\n",
    "    letters_folder = path_to(dataset_name)\n",
    "\n",
    "    if not os.path.exists(letters_folder):\n",
    "        with tarfile.open(filename) as tar:\n",
    "            tar.extractall(path=top_data_folder)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "download_and_extract(dataset_small)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "download_and_extract(dataset_large)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "image_size = 28\n",
    "class_count = 10\n",
    "image_channels = 1\n",
    "\n",
    "def load_folder(folder):\n",
    "    image_filenames = [f for f in os.listdir(folder) if not os.path.isdir(os.path.join(folder, f))]\n",
    "    image_count = len(image_filenames)\n",
    "    \n",
    "    dataset = np.ndarray(shape=(image_count, image_size, image_size, image_channels), dtype=np.float32)\n",
    "    \n",
    "    image_counter = 0\n",
    "    \n",
    "    for f in image_filenames:\n",
    "        image_filename = os.path.join(folder, f)\n",
    "        try:\n",
    "            image_data = np.expand_dims(ndimage.imread(image_filename).astype(float), axis=2)\n",
    "            dataset[image_counter, :, :] = image_data\n",
    "            image_counter += 1\n",
    "        except IOError as e:\n",
    "            # There are plenty of images, I skip this one\n",
    "            pass\n",
    "        \n",
    "    dataset = dataset[0:image_counter, :, :]\n",
    "    \n",
    "    print('Full dataset tensor for %s:' % folder, dataset.shape)\n",
    "    \n",
    "    return dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def get_complete_dataset(dataset_name):\n",
    "    image_depth = 256\n",
    "    \n",
    "    pickle_file = path_to(dataset_name + '_conv.pickle')\n",
    "    \n",
    "    if not os.path.exists(pickle_file):\n",
    "        image_pixels = []\n",
    "        image_labels = []\n",
    "        image_labels_one_hot = []\n",
    "        \n",
    "        for i, letter in enumerate(\"ABCDEFGHIJ\"):\n",
    "            letter_dataset = (load_folder(path_to(os.path.join(dataset_name, letter))) * 2 - image_depth) / image_depth\n",
    "            \n",
    "            letter_image_count = len(letter_dataset)\n",
    "            \n",
    "            letter_labels = np.ones(shape=(letter_image_count,), dtype=np.float32) * i\n",
    "            letter_labels_one_hot = np.ndarray(shape=(letter_image_count, class_count), dtype=np.float32)\n",
    "            \n",
    "            label_one_hot = np.zeros(shape = (class_count,), dtype=np.float32)\n",
    "            label_one_hot[i] = 1.0\n",
    "            letter_labels_one_hot[:] = label_one_hot\n",
    "            \n",
    "            image_pixels.append(letter_dataset)\n",
    "            image_labels.append(letter_labels)\n",
    "            image_labels_one_hot.append(letter_labels_one_hot)\n",
    "            \n",
    "        dataset = { 'pixels': np.concatenate(image_pixels), \n",
    "                    'labels': np.concatenate(image_labels),\n",
    "                    'labels_one_hot': np.concatenate(image_labels_one_hot) }\n",
    "            \n",
    "        with open(pickle_file, 'wb') as f:\n",
    "            pickle.dump(dataset, f, pickle.HIGHEST_PROTOCOL)\n",
    "            \n",
    "        return dataset\n",
    "    else:\n",
    "        with open(pickle_file, 'rb') as f:\n",
    "            return pickle.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def split(dataset_name, test_size=0.2, validate_size=0.25, random_state=42):\n",
    "    dataset = get_complete_dataset(dataset_name)\n",
    "    answer = {}\n",
    "    \n",
    "    sss1 = StratifiedShuffleSplit(n_splits=1, test_size=test_size, random_state=random_state)\n",
    "    train_validate_indices, test_indices = list(sss1.split(dataset['pixels'], dataset['labels']))[0]\n",
    "\n",
    "    answer['X_test'] = dataset['pixels'][test_indices]\n",
    "    answer['y_test'] = dataset['labels_one_hot'][test_indices]\n",
    "\n",
    "    train_validate_pixels = dataset['pixels'][train_validate_indices]\n",
    "    train_validate_labels = dataset['labels'][train_validate_indices]\n",
    "    train_validate_labels_one_hot = dataset['labels_one_hot'][train_validate_indices]\n",
    "    \n",
    "    sss2 = StratifiedShuffleSplit(n_splits=1, test_size=validate_size, random_state=random_state * random_state)\n",
    "    train_indices, validate_indices = list(sss2.split(train_validate_pixels, train_validate_labels_one_hot))[0]\n",
    "    \n",
    "    answer['X_train'] = train_validate_pixels[train_indices]\n",
    "    answer['y_train'] = train_validate_labels_one_hot[train_indices]\n",
    "    \n",
    "    answer['X_validate'] = train_validate_pixels[validate_indices]\n",
    "    answer['y_validate'] = train_validate_labels_one_hot[validate_indices]\n",
    "    \n",
    "    return answer    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "split_dataset = split(dataset_small)\n",
    "#split_dataset = split(dataset_large)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "('X_test', (3745, 28, 28, 1))\n",
      "('X_train', (11234, 28, 28, 1))\n",
      "('X_validate', (3745, 28, 28, 1))\n",
      "('y_validate', (3745, 10))\n",
      "('y_train', (11234, 10))\n",
      "('y_test', (3745, 10))\n"
     ]
    }
   ],
   "source": [
    "for k in split_dataset.keys():\n",
    "    print(k, split_dataset[k].shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "batch_size = 32\n",
    "\n",
    "patch_size_1 = 7\n",
    "depth_1 = 16\n",
    "\n",
    "patch_size_2 = 5\n",
    "depth_2 = 8\n",
    "\n",
    "stride_1 = 2\n",
    "stride_2 = 2\n",
    "\n",
    "strides_1 = [1, stride_1, stride_1, 1]\n",
    "strides_2 = [1, stride_2, stride_2, 1]\n",
    "\n",
    "hidden_node_count= 64\n",
    "\n",
    "learning_rate = 0.01"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "graph = tf.Graph()\n",
    "\n",
    "with graph.as_default():\n",
    "    # Placeholders for train datasets/labels\n",
    "    tf_train_dataset = tf.placeholder(tf.float32, shape=(batch_size, image_size, image_size, image_channels),\n",
    "                                     name='train_dataset')\n",
    "    tf_train_labels = tf.placeholder(tf.float32, shape=(batch_size, class_count), name='train_labels')\n",
    "    \n",
    "    # Constants for validate/test data\n",
    "    tf_validate_dataset = tf.constant(split_dataset['X_validate'], name='validate_dataset')\n",
    "    tf_validate_labels = tf.constant(split_dataset['y_validate'], name='validate_labels')\n",
    "    tf_test_dataset = tf.constant(split_dataset['X_test'], name='test_dataset')\n",
    "    tf_test_labels = tf.constant(split_dataset['y_test'], name='test_labels')\n",
    "    \n",
    "    # Variables\n",
    "    layer_1_conv_filter = tf.Variable(tf.truncated_normal([patch_size_1, patch_size_1, image_channels, depth_1], stddev=0.1),\n",
    "                                     name='layer_1_conv_filter')\n",
    "    layer_1_biases = tf.Variable(tf.zeros([depth_1]), name='layer_1_biases')\n",
    "    layer_2_conv_filter = tf.Variable(tf.truncated_normal([patch_size_2, patch_size_2, depth_1, depth_2], stddev=0.1), \n",
    "                                      name='layer_2_conv_filter')\n",
    "    layer_2_biases = tf.Variable(tf.ones([depth_2]), name='layer_2_biases')\n",
    "    \n",
    "    layer_1_output_size = image_size // stride_1\n",
    "    layer_2_output_size = layer_1_output_size // stride_2\n",
    "    \n",
    "    layer_3_weights = tf.Variable(tf.truncated_normal([layer_2_output_size * layer_2_output_size * depth_2, hidden_node_count],\n",
    "                                                     stddev=0.1), name='layer_3_weights')\n",
    "    layer_3_biases = tf.Variable(tf.ones([hidden_node_count]), name='layer_3_biases')\n",
    "    \n",
    "    layer_4_weights = tf.Variable(tf.truncated_normal([hidden_node_count, class_count], stddev=0.1), name='layer_4_weights')\n",
    "    layer_4_biases = tf.Variable(tf.ones(class_count), name='layer_4_biases')\n",
    "    \n",
    "    # Model\n",
    "    def model(data):\n",
    "        conv1 = tf.nn.conv2d(data, layer_1_conv_filter, strides_1, padding='SAME')\n",
    "        hidden1 = tf.nn.relu(conv1 + layer_1_biases)\n",
    "        conv2 = tf.nn.conv2d(hidden1, layer_2_conv_filter, strides_2, padding='SAME')\n",
    "        hidden2 = tf.nn.relu(conv2 + layer_2_biases)\n",
    "        hidden2_shape = hidden2.get_shape().as_list()\n",
    "        hidden2_reshaped = tf.reshape(hidden2, [hidden2_shape[0], hidden2_shape[1] * hidden2_shape[2] * hidden2_shape[3]])\n",
    "        hidden3 = tf.nn.relu(tf.matmul(hidden2_reshaped, layer_3_weights) + layer_3_biases)\n",
    "        output = tf.nn.relu(tf.matmul(hidden3, layer_4_weights) + layer_4_biases)\n",
    "        return output\n",
    "       \n",
    "    # Train\n",
    "    logits_train = model(tf_train_dataset)\n",
    "    loss = tf.reduce_mean(tf.nn.softmax_cross_entropy_with_logits(logits=logits_train, labels=tf_train_labels))\n",
    "\n",
    "    optimizer = tf.train.GradientDescentOptimizer(learning_rate).minimize(loss)\n",
    "  \n",
    "    # Predictions\n",
    "    prediction_train = tf.nn.softmax(logits_train)\n",
    "    prediction_validate = tf.nn.softmax(model(tf_validate_dataset))\n",
    "    prediction_test = tf.nn.softmax(model(tf_test_dataset))\n",
    "    \n",
    "    def accuracy(predictions, labels):\n",
    "        correct_predictions = tf.equal(tf.argmax(predictions, 1), tf.argmax(labels, 1))\n",
    "        return 100.0 * tf.reduce_mean(tf.cast(correct_predictions, tf.float32))\n",
    "    \n",
    "    # Accuracies\n",
    "    accuracy_train = accuracy(prediction_train, tf_train_labels)\n",
    "    accuracy_validate = accuracy(prediction_validate, tf_validate_labels)\n",
    "    accuracy_test = accuracy(prediction_test, tf_test_labels)\n",
    "    \n",
    "    # Summaries\n",
    "    tf.scalar_summary(\"Minibatch Loss\", loss)\n",
    "    tf.scalar_summary(\"Minibatch Accuracy\", accuracy_validate)\n",
    "    tf.scalar_summary(\"Validate Accuracy\", accuracy_validate)\n",
    "    tf.scalar_summary(\"Test Accuracy\", accuracy_test)\n",
    "    \n",
    "    summary_op = tf.merge_all_summaries()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": false,
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Initialized\n",
      "Minibatch loss at step 0: 3.068870\n",
      "Minibatch accuracy: 15.62%\n",
      "Validation accuracy: 11.03%\n",
      "Minibatch loss at step 100: 1.210921\n",
      "Minibatch accuracy: 68.75%\n",
      "Validation accuracy: 64.54%\n",
      "Minibatch loss at step 200: 0.429501\n",
      "Minibatch accuracy: 84.38%\n",
      "Validation accuracy: 77.12%\n",
      "Minibatch loss at step 300: 0.448068\n",
      "Minibatch accuracy: 87.50%\n",
      "Validation accuracy: 81.42%\n",
      "Minibatch loss at step 400: 0.874682\n",
      "Minibatch accuracy: 78.12%\n",
      "Validation accuracy: 77.41%\n",
      "Minibatch loss at step 500: 0.626475\n",
      "Minibatch accuracy: 78.12%\n",
      "Validation accuracy: 85.37%\n",
      "Minibatch loss at step 600: 0.470368\n",
      "Minibatch accuracy: 81.25%\n",
      "Validation accuracy: 85.87%\n",
      "Minibatch loss at step 700: 0.420973\n",
      "Minibatch accuracy: 90.62%\n",
      "Validation accuracy: 86.09%\n",
      "Minibatch loss at step 800: 0.468707\n",
      "Minibatch accuracy: 84.38%\n",
      "Validation accuracy: 85.47%\n",
      "Minibatch loss at step 900: 0.218820\n",
      "Minibatch accuracy: 93.75%\n",
      "Validation accuracy: 85.07%\n",
      "Minibatch loss at step 1000: 0.387154\n",
      "Minibatch accuracy: 93.75%\n",
      "Validation accuracy: 86.01%\n",
      "Minibatch loss at step 1100: 1.101432\n",
      "Minibatch accuracy: 71.88%\n",
      "Validation accuracy: 86.70%\n",
      "Minibatch loss at step 1200: 0.633229\n",
      "Minibatch accuracy: 84.38%\n",
      "Validation accuracy: 87.80%\n",
      "Minibatch loss at step 1300: 0.521483\n",
      "Minibatch accuracy: 90.62%\n",
      "Validation accuracy: 88.17%\n",
      "Minibatch loss at step 1400: 0.317284\n",
      "Minibatch accuracy: 93.75%\n",
      "Validation accuracy: 88.30%\n",
      "Minibatch loss at step 1500: 0.359576\n",
      "Minibatch accuracy: 90.62%\n",
      "Validation accuracy: 87.90%\n",
      "Minibatch loss at step 1600: 0.290492\n",
      "Minibatch accuracy: 90.62%\n",
      "Validation accuracy: 87.42%\n",
      "Minibatch loss at step 1700: 0.276025\n",
      "Minibatch accuracy: 96.88%\n",
      "Validation accuracy: 88.04%\n",
      "Minibatch loss at step 1800: 0.827327\n",
      "Minibatch accuracy: 81.25%\n",
      "Validation accuracy: 88.09%\n",
      "Minibatch loss at step 1900: 0.688007\n",
      "Minibatch accuracy: 81.25%\n",
      "Validation accuracy: 88.38%\n",
      "Minibatch loss at step 2000: 0.409373\n",
      "Minibatch accuracy: 93.75%\n",
      "Validation accuracy: 88.97%\n",
      "Minibatch loss at step 2100: 0.324966\n",
      "Minibatch accuracy: 93.75%\n",
      "Validation accuracy: 89.16%\n",
      "Minibatch loss at step 2200: 0.460954\n",
      "Minibatch accuracy: 90.62%\n",
      "Validation accuracy: 88.62%\n",
      "Minibatch loss at step 2300: 0.321713\n",
      "Minibatch accuracy: 93.75%\n",
      "Validation accuracy: 88.52%\n",
      "Minibatch loss at step 2400: 0.341484\n",
      "Minibatch accuracy: 100.00%\n",
      "Validation accuracy: 88.95%\n",
      "Minibatch loss at step 2500: 0.602128\n",
      "Minibatch accuracy: 84.38%\n",
      "Validation accuracy: 88.97%\n",
      "Minibatch loss at step 2600: 0.759574\n",
      "Minibatch accuracy: 75.00%\n",
      "Validation accuracy: 88.89%\n",
      "Minibatch loss at step 2700: 0.360075\n",
      "Minibatch accuracy: 93.75%\n",
      "Validation accuracy: 89.85%\n",
      "Minibatch loss at step 2800: 0.312170\n",
      "Minibatch accuracy: 90.62%\n",
      "Validation accuracy: 89.56%\n",
      "Minibatch loss at step 2900: 0.524653\n",
      "Minibatch accuracy: 90.62%\n",
      "Validation accuracy: 89.37%\n",
      "Minibatch loss at step 3000: 0.371673\n",
      "Minibatch accuracy: 93.75%\n",
      "Validation accuracy: 89.05%\n",
      "Minibatch loss at step 3100: 0.275620\n",
      "Minibatch accuracy: 100.00%\n",
      "Validation accuracy: 89.35%\n",
      "Minibatch loss at step 3200: 0.433749\n",
      "Minibatch accuracy: 90.62%\n",
      "Validation accuracy: 89.64%\n",
      "Minibatch loss at step 3300: 0.576743\n",
      "Minibatch accuracy: 84.38%\n",
      "Validation accuracy: 89.72%\n",
      "Minibatch loss at step 3400: 0.562248\n",
      "Minibatch accuracy: 90.62%\n",
      "Validation accuracy: 89.96%\n",
      "Minibatch loss at step 3500: 0.459006\n",
      "Minibatch accuracy: 90.62%\n",
      "Validation accuracy: 90.28%\n",
      "Minibatch loss at step 3600: 0.587440\n",
      "Minibatch accuracy: 87.50%\n",
      "Validation accuracy: 89.99%\n",
      "Minibatch loss at step 3700: 0.399092\n",
      "Minibatch accuracy: 93.75%\n",
      "Validation accuracy: 89.51%\n",
      "Minibatch loss at step 3800: 0.323988\n",
      "Minibatch accuracy: 93.75%\n",
      "Validation accuracy: 89.91%\n",
      "Minibatch loss at step 3900: 0.344793\n",
      "Minibatch accuracy: 93.75%\n",
      "Validation accuracy: 90.68%\n",
      "Minibatch loss at step 4000: 0.551676\n",
      "Minibatch accuracy: 84.38%\n",
      "Validation accuracy: 90.52%\n",
      "Minibatch loss at step 4100: 0.655504\n",
      "Minibatch accuracy: 90.62%\n",
      "Validation accuracy: 90.68%\n",
      "Minibatch loss at step 4200: 0.546145\n",
      "Minibatch accuracy: 90.62%\n",
      "Validation accuracy: 90.79%\n",
      "Minibatch loss at step 4300: 0.476712\n",
      "Minibatch accuracy: 90.62%\n",
      "Validation accuracy: 90.41%\n",
      "Minibatch loss at step 4400: 0.384520\n",
      "Minibatch accuracy: 93.75%\n",
      "Validation accuracy: 89.80%\n",
      "Minibatch loss at step 4500: 0.391003\n",
      "Minibatch accuracy: 90.62%\n",
      "Validation accuracy: 89.96%\n",
      "Minibatch loss at step 4600: 0.302792\n",
      "Minibatch accuracy: 93.75%\n",
      "Validation accuracy: 91.05%\n",
      "Minibatch loss at step 4700: 0.552669\n",
      "Minibatch accuracy: 87.50%\n",
      "Validation accuracy: 90.79%\n",
      "Minibatch loss at step 4800: 0.576636\n",
      "Minibatch accuracy: 90.62%\n",
      "Validation accuracy: 91.19%\n",
      "Minibatch loss at step 4900: 0.600075\n",
      "Minibatch accuracy: 90.62%\n",
      "Validation accuracy: 91.19%\n",
      "Minibatch loss at step 5000: 0.451722\n",
      "Minibatch accuracy: 93.75%\n",
      "Validation accuracy: 90.97%\n",
      "Test accuracy: 89.67%\n"
     ]
    }
   ],
   "source": [
    "num_steps = 5001\n",
    "\n",
    "with tf.Session(graph=graph) as session:\n",
    "    writer = tf.train.SummaryWriter(logs_folder, graph=graph)\n",
    "    \n",
    "    session.run(tf.initialize_all_variables())\n",
    "    print(\"Initialized\")\n",
    "    \n",
    "    for step in range(num_steps):\n",
    "        offset = (step * batch_size) % (split_dataset['y_train'].shape[0] - batch_size)\n",
    "    \n",
    "        batch_data = split_dataset['X_train'][offset:(offset + batch_size), :]\n",
    "        batch_labels = split_dataset['y_train'][offset:(offset + batch_size), :]\n",
    "    \n",
    "        feed_dict = {tf_train_dataset : batch_data, tf_train_labels : batch_labels}\n",
    "        _, l, summary, acc_tr, acc_val, acc_te = session.run(\n",
    "            [optimizer, loss, summary_op, \n",
    "             accuracy_train, accuracy_validate, accuracy_test], \n",
    "            feed_dict=feed_dict)\n",
    "        \n",
    "        writer.add_summary(summary, step)\n",
    "        \n",
    "        if (step % 100 == 0):\n",
    "            print(\"Minibatch loss at step %d: %f\" % (step, l))\n",
    "            print(\"Minibatch accuracy: %.2f%%\" % acc_tr)\n",
    "            print(\"Validation accuracy: %.2f%%\" % acc_val)\n",
    "    print(\"Test accuracy: %.2f%%\" % acc_te)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
